{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spatial Regression in Python\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Standing on the shoulders of giants**: This tutorial is based on excellent open source materials developed by Daniel Arribas-Bel (Uni. Liverpool) available in [here](http://darribas.org/gds_scipy16/ipynb_md/08_spatial_regression.html) (licensed with Creative Commons BY-NC-SA). Inspiration is also driven from Chapter 11 of the forthcoming book \"Geographic Data Science with Python\" by Rey, Arribas-Bel & Wolf which you can access from [here](https://geographicdata.science/book/notebooks/11_regression.html) (licensed with Creative Commons BY-NC-ND)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook covers a brief and gentle introduction to spatial econometrics in Python. To do that, we will use a set of Austin properties listed in AirBnb.\n",
        "\n",
        "The core idea of spatial econometrics is to introduce a formal representation of space into the statistical framework for regression. This can be done in many ways: by including predictors based on space (e.g. distance to relevant features), by splitting the datasets into subsets that map into different geographical regions (e.g. [spatial regimes](http://pysal.readthedocs.io/en/latest/library/spreg/regimes.html)), by exploiting close distance to other observations to borrow information in the estimation (e.g. [kriging](https://en.wikipedia.org/wiki/Kriging)), or by introducing variables that put in relation their value at a given location with those in nearby locations, to give a few examples. Some of these approaches can be implemented with standard non-spatial techniques, while others require bespoke models that can deal with the issues introduced. In this short tutorial, we will focus on the latter group. In particular, we will introduce some of the most commonly used methods in the field of spatial econometrics.\n",
        "\n",
        "The example we will use to demonstrate this draws on hedonic house price modelling. This a well-established methodology that was developed by [Rosen (1974)](https://www.sonoma.edu/users/c/cuellar/econ421/rosen-hedonic.pdf) that is capable of recovering the marginal willingness to pay for goods or services that are not traded in the market. In other words, this allows us to put an implicit price on things such as living close to a park or in a neighborhood with good quality of air. In addition, since hedonic models are based on linear regression, the technique can also be used to obtain predictions of house prices.\n",
        "\n",
        "## Prepare data\n",
        "\n",
        "Before anything, let us load up the libraries we will use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pysal.model import spreg\n",
        "from pysal.lib import weights\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import seaborn as sns\n",
        "import osmnx as ox\n",
        "import folium\n",
        "sns.set(style=\"whitegrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's read the Airbnb data and OSM data for Austin, Texas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read listings\n",
        "fp = \"data/listings.csv\"\n",
        "data = pd.read_csv(fp)\n",
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read OSM data - get administrative boundaries\n",
        "\n",
        "# define the place query\n",
        "query = {'city': 'Austin'}\n",
        "\n",
        "# get the boundaries of the place (add additional buffer around the query)\n",
        "boundaries = ox.geocode_to_gdf(query, buffer_dist=5000)\n",
        "\n",
        "# Let's check the boundaries on a map\n",
        "boundaries.explore()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's convert the Airbnb data into GeoDataFrame based on the `longitude` and `latitude` columns and filter the data geographically based on Austing boundaries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a GeoDataFrame\n",
        "data[\"geometry\"] = gpd.points_from_xy(data[\"longitude\"], data[\"latitude\"])\n",
        "data = gpd.GeoDataFrame(data, crs=\"epsg:4326\")\n",
        "\n",
        "# Filter data geographically\n",
        "data = gpd.sjoin(data, boundaries[[\"geometry\"]])\n",
        "data = data.reset_index(drop=True)\n",
        "\n",
        "# Check the first rows\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One of the most interesting attributes in our data is naturally **price**: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data[\"price\"].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, our values are represented as strings with a dollar sign. Before we can take a logarithmic value out of them, we need to remove the dollar sign and convert the values to floats:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove dollar sign and the thousand separator (comma, e.g. 1000,000.00) and convert to float\n",
        "data[\"price\"] = data[\"price\"].str.replace(\"$\", '', regex=True).str.replace(\",\", \"\").astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's investigate our listings data on top of a map:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here the tooltip parameter specifies which attributes are shown when hovering on top of the points\n",
        "# The vmax parameter specifies the maximum value for the colormap (here, all 1000 dollars and above are combined)\n",
        "data.explore(column=\"price\", cmap=\"Reds\", scheme=\"quantiles\", k=4, tooltip=[\"name\", \"price\"], vmax=1000, tiles=\"CartoDB Positron\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline (nonspatial) regression\n",
        "\n",
        "Before introducing explicitly spatial methods, we will run a simple linear regression model. This will allow us, on the one hand, set the main principles of hedonic modeling and how to interpret the coefficients, which is good because the spatial models will build on this; and, on the other hand, it will provide a baseline model that we can use to evaluate how meaningful the spatial extensions are.\n",
        "\n",
        "Essentially, the core of a linear regression is to explain a given variable -the price of a listing $i$ on AirBnb ($P_i$)- as a linear function of a set of other characteristics we will collectively call $X_i$:\n",
        "\n",
        "$$\n",
        "\\ln(P_i) = \\alpha + \\beta X_i + \\epsilon_i\n",
        "$$\n",
        "\n",
        "For several reasons, it is common practice to introduce the price in logarithms, so we will do so here. Additionally, since this is a probabilistic model, we add an error term $\\epsilon_i$ that is assumed to be well-behaved (i.i.d. as a normal).\n",
        "\n",
        "For our example, we will consider the following set of explanatory features of each listed property:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "explanatory_vars = ['host_listings_count', 'bathrooms', 'bedrooms', 'beds', 'guests_included']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Additionally, we are going to derive a new feature of a listing from the amenities variable. Let us construct a variable that takes 1 if the listed property has a pool and 0 otherwise:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def has_pool(a):\n",
        "    if 'Pool' in a:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "    \n",
        "data['pool'] = data['amenities'].apply(has_pool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's then calculate the logarithmic value of the price:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data[\"log_price\"] = np.log(data[\"price\"] + 0.000001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Do we have any missing values in our dependent or explanatory variables?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_model_attributes = [\"price\"] + explanatory_vars\n",
        "has_nans = False\n",
        "for attr in all_model_attributes:\n",
        "    if data[attr].hasnans:\n",
        "        has_nans = True\n",
        "print(\"Has missing values:\", has_nans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Okay, as we can see there are missing values, hence, let's remove them before continuing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = data.dropna(subset=all_model_attributes).copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To run the model, we can use the `spreg` module in `PySAL`, which implements a standard OLS routine, but is particularly well suited for regressions on spatial data. Also, although for the initial model we do not need it, let us build a spatial weights matrix that connects every observation to its 8 nearest neighbors. This will allow us to get extra diagnostics from the baseline model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "w = weights.KNN.from_dataframe(data, k=8)\n",
        "w.transform = 'R'\n",
        "w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At this point, we are ready to fit the regression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m1 = spreg.OLS(data[['log_price']].values, data[explanatory_vars].values, \n",
        "                  name_y = 'log_price', name_x = explanatory_vars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get a quick glimpse of the results, we can print its summary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(m1.summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results are largely unsurprising, but nonetheless reassuring. Both an extra bedroom and an extra bathroom increase the final price around 30%. Accounting for those, an extra bed pushes the price about 2%. Neither the number of guests included nor the number of listings the host has in total have a significant effect on the final price.\n",
        "\n",
        "Including a spatial weights object in the regression buys you an extra bit: the summary provides results on the diagnostics for spatial dependence. These are a series of statistics that test whether the residuals of the regression are spatially correlated, against the null of a random distribution over space. If the latter is rejected a key assumption of OLS, independently distributed error terms, is violated. Depending on the structure of the spatial pattern, different strategies have been defined within the spatial econometrics literature to deal with them. The main summary from the diagnostics for spatial dependence is that there is clear evidence to reject the null of spatial randomness in the residuals, hence an explicitly spatial approach is warranted.\n",
        "\n",
        "## Spatially lagged exogenous regressors (`WX`)\n",
        "\n",
        "The first and most straightforward way to introduce space is by **\"spatially lagging\" one of the explanatory variables**. Mathematically, this can be expressed as follows:\n",
        "\n",
        "$$\n",
        "\\ln(P_i) = \\alpha + \\beta X_i + \\delta \\sum_j w_{ij} X'_i + \\epsilon_i\n",
        "$$\n",
        "\n",
        "where $\\ln(P_i)$ is our dependent variable (logarithmic price), $X'_i$ is a subset of $X_i$, although it could encompass all of the explanatory variables, and $w_{ij}$ is the $ij$-th cell of a spatial weights matrix $W$. Because $W$ assigns non-zero values only to spatial neighbors, if $W$ is row-standardized (customary in this context), then $\\sum_j w_{ij} X'_i$ captures the average value of $X'_i$ in the surroundings of location $i$. This is what we call the *spatial lag* of $X_i$. Also, since it is a spatial transformation of an explanatory variable, the standard estimation approach -OLS- is sufficient: spatially lagging the variables does not violate any of the assumptions on which OLS relies.\n",
        "\n",
        "Usually, we will want to spatially lag variables that we think may affect the price of a house in a given location. For example, one could think that pools represent a visual amenity. If that is the case, then listed properties surrounded by other properties with pools might, everything else equal, be more expensive. To calculate the number of pools surrounding each property, we can build an alternative weights matrix that we do not row-standardize:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create weigts\n",
        "w_pool = weights.KNN.from_dataframe(data, k=8)\n",
        "# Assign spatial lag based on the pool values\n",
        "lagged = data.assign(w_pool=weights.spatial_lag.lag_spatial(w_pool, data['pool'].values))\n",
        "lagged.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now we can run the model, which has the same setup as `m1`, with the exception that it includes the number of AirBnb properties with pools surrounding each house:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add pool to the explanatory variables\n",
        "extended_vars = explanatory_vars + [\"pool\", \"w_pool\"]\n",
        "\n",
        "m2 = spreg.OLS(lagged[['log_price']].values, lagged[extended_vars].values, \n",
        "               name_y = 'log_price', name_x = extended_vars)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(m2.summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results are largely consistent with the original model. Also, incidentally, the number of pools surrounding a property does not appear to have any significant effect on the price of a given property. This could be for a host of reasons: maybe AirBnb customers do not value the number of pools surrounding a property where they are looking to stay; but maybe they do but our dataset only allows us to capture the number of pools in *other* AirBnb properties, which is not necessarily a good proxy of the number of pools in the immediate surroundings of a given property.\n",
        "\n",
        "## Spatially lagged endogenous regressors (`WY`)\n",
        "\n",
        "In a similar way to how we have included the spatial lag, one could think the prices of houses surrounding a given property also enter its own price function. In math terms, this implies the following:\n",
        "\n",
        "$$\n",
        "\\ln(P_i) = \\alpha + \\lambda \\sum_j w_{ij} \\ln(P_i) + \\beta X_i + \\epsilon_i\n",
        "$$\n",
        "\n",
        "This is essentially what we call a *spatial lag* model in spatial econometrics. Two calls for caution:\n",
        "\n",
        "1. Unlike before, this specification *does* violate some of the assumptions on which OLS relies. In particular, it is including an endogenous variable $\\ln(P_i)$ on the right-hand side. This means we need a new estimation method to obtain reliable coefficients. The technical details of this go well beyond the scope of this tutorial. But we can offload those to `PySAL` and use the `GM_Lag` class, which implements the state-of-the-art approach to estimate this model.\n",
        "1. A more conceptual *gotcha*: you might be tempted to read the equation above as the effect of the price in neighboring locations $j$ on that of location $i$. This is not exactly the exact interpretation. Instead, we need to realize this is all assumed to be a \"joint decission\": rather than some houses setting their price first and that having a subsequent effect on others, what the equation models is an interdependent process by which each owner sets her own price *taking into account* the price that will be set in neighboring locations. This might read a bit like a technical subtlety and, to some extent, it is; but it is important to keep it in mind when you are interpreting the results.\n",
        "\n",
        "Let us see how you would run this using `PySAL`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "variables = explanatory_vars + [\"pool\"]\n",
        "m3 = spreg.GM_Lag(data[['log_price']].values, data[variables].values, \n",
        "                  w=w,\n",
        "                  name_y = 'ln(price)', name_x = variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(m3.summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, results are again very similar in all the other variable. It is also very clear that the estimate of the spatial lag of price is statistically significant. This points to evidence that there are processes of spatial interaction between property owners when they set their price.\n",
        "\n",
        "## Prediction performance of spatial models\n",
        "\n",
        "Even if we are not interested in the interpretation of the model to learn more about how alternative factors determine the price of an AirBnb property, spatial econometrics can be useful. In a purely predictive setting, the use of explicitly spatial models is likely to improve accuracy in cases where space plays a key role in the data generating process. To have a quick look at this issue, we can use the mean squared error (MSE), a standard metric of accuracy in the machine learning literature, to evaluate whether explicitly spatial models are better than traditional, non-spatial ones (the smaller the value, the better):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error as mse\n",
        "\n",
        "mses = pd.Series({'OLS': mse(data[\"log_price\"], m1.predy.flatten()), \\\n",
        "                  'OLS+W': mse(data[\"log_price\"], m2.predy.flatten()), \\\n",
        "                  'Lag': mse(data[\"log_price\"], m3.predy_e)\n",
        "                    })\n",
        "mses.sort_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the inclusion of the number of surrounding pools slightly reduces the MSE, and the inclusion of the spatial lag of price improves the accuracy of the model even further."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.8 ('traillop')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "vscode": {
      "interpreter": {
        "hash": "31733d31b68f2bf33a40ef3744aa6e6dabc719cffbfa95fe2fcbd4a0d9e78042"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
